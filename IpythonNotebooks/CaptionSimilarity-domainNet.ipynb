{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe2bf34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "# import os\n",
    "import inflect\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "596d0c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pluralize(word):\n",
    "    p = inflect.engine()\n",
    "    return p.plural(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ff2c38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "084f1d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_prefix(caption):\n",
    "    \n",
    "    if caption.startswith(\"An image of a \"):\n",
    "        caption = caption.split(\"An image of a \")[-1]\n",
    "    elif caption.startswith(\"An image of \"):\n",
    "        caption = caption.split(\"An image of \")[-1]\n",
    "    elif caption.startswith(\"An image of... \"):\n",
    "        caption = caption.split(\"An image of... \")[-1]\n",
    "        \n",
    "#     if caption.startswith(\"Sure\"):\n",
    "#         print(caption)\n",
    "#         print(\"&\"*100)\n",
    "\n",
    "    return caption\n",
    "\n",
    "def strip_classname(caption):\n",
    "    \n",
    "    if caption.startswith(\"Possible category: \"):\n",
    "        caption = caption.split(\"Possible category: \")[-1] \n",
    "    elif caption.startswith(\"Sure!\"):\n",
    "        caption = caption.rsplit(\"\\n\",1)[-1].strip()\n",
    "        \n",
    "    if \"\\n\" in caption:\n",
    "        caption = caption.rsplit(\"\\n\",1)[-1]\n",
    "        \n",
    "    return caption\n",
    "\n",
    "def get_classdesc(classnames, json_dict):\n",
    "    descriptions = []\n",
    "    for c in classnames:\n",
    "        name = c.replace(\"+\", \" \").replace(\"_\", \" \")\n",
    "        desc = \" or \".join(json_dict[c])\n",
    "        name_desc = \"{} with {}\".format(name, desc)\n",
    "        descriptions.append(name_desc)\n",
    "    return descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1cefec",
   "metadata": {},
   "source": [
    "### Caption - label similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c33b8a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(caption_embeddings, class_embeddings, topk=5):\n",
    "    similarity_matrix = util.cos_sim(caption_embeddings, class_embeddings)\n",
    "    pseudoLabel = similarity_matrix.topk(topk, 1).indices    \n",
    "    gt_label = torch.Tensor([id_to_classid[fid] for fid in all_flickrids]).view(-1,1).repeat(1,topk)\n",
    "    matched_labels = (torch.Tensor(pseudoLabel) == torch.Tensor(gt_label))\n",
    "    top1_acc = matched_labels[:,0].sum()/len(gt_label)\n",
    "    topk_acc = matched_labels.any(1).sum()/len(gt_label)\n",
    "    return top1_acc, topk_acc, similarity_matrix.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2031dcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## category\n",
    "meta = json.load(open(\"../metadata/officehome.json\"))\n",
    "\n",
    "# cats = {int(c[\"category_id\"]):c[\"category_name\"] for c in meta[\"categories\"]}\n",
    "cats = [cats[idx].replace(\"_\", \" \").lower() for idx in range(65)]\n",
    "# cats = [cats[idx].replace(\",\",\"\") for idx in range(68)]\n",
    "# cats = [cats[idx].replace(\"_indoor\",\"\").replace(\"_outdoor\",\"\").replace(\"_\",\" \") for idx in range(205)]\n",
    "# cats = [cats[idx].replace(\"_\",\" \") for idx in range(205)]\n",
    "# cats = [cats[idx].replace(\"+\",\" \").replace(\"_\",\" \").replace(\"-\",\" \") for idx in range(345)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab22c5be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['drill',\n",
       " 'exit sign',\n",
       " 'bottle',\n",
       " 'glasses',\n",
       " 'computer',\n",
       " 'file cabinet',\n",
       " 'shelf',\n",
       " 'toys',\n",
       " 'sink',\n",
       " 'laptop',\n",
       " 'kettle',\n",
       " 'folder',\n",
       " 'keyboard',\n",
       " 'flipflops',\n",
       " 'pencil',\n",
       " 'bed',\n",
       " 'hammer',\n",
       " 'toothbrush',\n",
       " 'couch',\n",
       " 'bike',\n",
       " 'postit notes',\n",
       " 'mug',\n",
       " 'webcam',\n",
       " 'desk lamp',\n",
       " 'telephone',\n",
       " 'helmet',\n",
       " 'mouse',\n",
       " 'pen',\n",
       " 'monitor',\n",
       " 'mop',\n",
       " 'sneakers',\n",
       " 'notebook',\n",
       " 'backpack',\n",
       " 'alarm clock',\n",
       " 'push pin',\n",
       " 'paper clip',\n",
       " 'batteries',\n",
       " 'radio',\n",
       " 'fan',\n",
       " 'ruler',\n",
       " 'pan',\n",
       " 'screwdriver',\n",
       " 'trash can',\n",
       " 'printer',\n",
       " 'speaker',\n",
       " 'eraser',\n",
       " 'bucket',\n",
       " 'chair',\n",
       " 'calendar',\n",
       " 'calculator',\n",
       " 'flowers',\n",
       " 'lamp shade',\n",
       " 'spoon',\n",
       " 'candles',\n",
       " 'clipboards',\n",
       " 'scissors',\n",
       " 'tv',\n",
       " 'curtains',\n",
       " 'fork',\n",
       " 'soda',\n",
       " 'table',\n",
       " 'knives',\n",
       " 'oven',\n",
       " 'refrigerator',\n",
       " 'marker']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06972acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_embeddings = model.encode(cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e91169f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65, 384)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec884997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "art:78.69/89.71\n",
      "art:78.69/89.71\n",
      "art:78.69/89.71\n",
      "product:88.66/96.15\n",
      "product:88.66/96.15\n",
      "product:88.66/96.15\n",
      "real:85.48/94.61\n",
      "real:85.48/94.61\n",
      "real:85.48/94.61\n",
      "clipart:73.46/87.80\n",
      "clipart:73.46/87.80\n",
      "clipart:73.46/87.80\n"
     ]
    }
   ],
   "source": [
    "# for dom in [\"clipart\", \"sketch\", \"painting\"]:\n",
    "for dom in ['art', 'product', 'real', 'clipart']:\n",
    "    for tgt in ['art', 'product', 'real', 'clipart']:\n",
    "        if dom == tgt:continue\n",
    "    \n",
    "        geo = meta['{}_train'.format(dom)]\n",
    "    #     id_to_cname = {ann[\"image_id\"]:ann[\"class_name\"].lower() for ann in geo[\"annotations\"]}\n",
    "        id_to_classid = {ann[\"image_id\"]:ann[\"category\"] for ann in geo[\"annotations\"]}\n",
    "        id_to_blip = {e[\"image_id\"]:e[\"blip2_cap\"] for e in geo[\"metadata\"]}\n",
    "\n",
    "        all_flickrids = list(id_to_blip.keys())\n",
    "        all_captions = [id_to_blip[v] for v in all_flickrids]\n",
    "        caption_embeddings = model.encode(all_captions)\n",
    "\n",
    "        top1, top5, pseudo = get_accuracy(caption_embeddings, class_embeddings)\n",
    "        print(\"{}:{:.2f}/{:.2f}\".format(dom, top1*100, top5*100))\n",
    "\n",
    "        with open(\"../hard_labels/officeHome_{}_{}_embedMatchPL.txt\".format(tgt, dom), \"w\") as fh:\n",
    "            write_str = \"\"\n",
    "            for fid, pl in zip(all_flickrids, pseudo):\n",
    "                write_str += \"{} {}\\n\".format(fid, pl)\n",
    "            fh.write(write_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76f8dcd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3975"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(caption_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3499c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for idx, cname in id_to_cname.items():\n",
    "    tag_set = set(id_to_blip[idx].split(\" \"))\n",
    "    cls = set(cname.split(\",\"))\n",
    "    cls = set(map(lambda v:v.replace(\" \",\"\"), cls))\n",
    "    cls_aug = set(map(lambda v:pluralize(v), cls))\n",
    "    cls = cls.union(cls_aug)\n",
    "\n",
    "    common = tag_set.intersection(cls)\n",
    "    \n",
    "    if len(common) == 0:\n",
    "        print(\"Tags:{}\".format(tag_set))\n",
    "        print(\"Class:{}\".format(cls))\n",
    "        print(common)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615b661b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d40ffb1",
   "metadata": {},
   "source": [
    "### Caption - Caption similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f3e42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "meta = json.load(open(\"/home/tarun/metadata/geoPlaces_metadata.json\"))\n",
    "cname_to_label = {c[\"category_name\"]:c[\"category_id\"] for c in meta[\"categories\"]}\n",
    "\n",
    "## usa\n",
    "cap = json.load(open(\"/home/tarun/llama/extracted_captions_geoplaces_usa.json\"))[\"extracted\"]\n",
    "id_to_cap = {e[\"flickr_id\"]:e[\"extracted_class_name\"] for e in cap}\n",
    "id_to_cap_usa = {k:strip_prefix(v) for k,v in id_to_cap.items()};\n",
    "id_to_label_usa = {e[\"flickr_id\"]:cname_to_label[e[\"gt_category\"]] for e in cap}\n",
    "\n",
    "## asia\n",
    "cap = json.load(open(\"/home/tarun/llama/extracted_captions_geoplaces_asia.json\"))[\"extracted\"]\n",
    "id_to_cap = {e[\"flickr_id\"]:e[\"extracted_class_name\"] for e in cap}\n",
    "id_to_cap_asia = {k:strip_prefix(v) for k,v in id_to_cap.items()};\n",
    "id_to_label_asia = {e[\"flickr_id\"]:cname_to_label[e[\"gt_category\"]] for e in cap}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00719bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_flickrids = list(id_to_cap_usa.keys())\n",
    "asia_flickrids = list(id_to_cap_asia.keys())\n",
    "\n",
    "usa_captions = [id_to_cap_usa[v] for v in usa_flickrids]\n",
    "usa_embeddings = model.encode(usa_captions)\n",
    "usa_labels = torch.tensor([id_to_label_usa[v] for v in usa_flickrids])\n",
    "\n",
    "asia_captions = [id_to_cap_asia[v] for v in asia_flickrids]\n",
    "asia_embeddings = model.encode(asia_captions)\n",
    "asia_labels = torch.Tensor([id_to_label_asia[v] for v in asia_flickrids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af96aed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity_acc(source_embed, target_embed, source_label, target_label, within=False, topks=[1,5]):\n",
    "    \n",
    "    topk = max(topks)\n",
    "        \n",
    "    if within:\n",
    "        similarity_matrix = util.cos_sim(source_embed, source_embed)\n",
    "        mostSimilar = similarity_matrix.topk(topk+1, 1).indices\n",
    "        mostSimilar = mostSimilar[:,1:]\n",
    "    else:\n",
    "        similarity_matrix = util.cos_sim(source_embed, target_embed)\n",
    "        mostSimilar = similarity_matrix.topk(topk, 1).indices\n",
    "    \n",
    "    similarLabels = torch.Tensor(target_label)[mostSimilar.long().reshape(-1)].reshape(-1, topk)\n",
    "    source_label = torch.Tensor(source_label).view(-1,1).repeat(1,topk)\n",
    "    \n",
    "    matched_labels = (torch.Tensor(similarLabels) == torch.Tensor(source_label))\n",
    "                      \n",
    "    top1_acc = matched_labels[:,0].sum()/len(source_label)\n",
    "    topk_acc = matched_labels.any(1).sum()/len(source_label)\n",
    "        \n",
    "    return top1_acc, topk_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27abdd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_similarity_acc(asia_embeddings, usa_embeddings, asia_labels, usa_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ff47c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix = util.cos_sim(asia_embeddings, usa_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cb9000",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = torch.sort(similarity_matrix, descending=True, dim=0).indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912a3688",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_orderedNeighbors = torch.Tensor(asia_labels)[ind[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba38abbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_orderedNeighbors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76f6c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "assigned_target_labels = torch.mode(k_orderedNeighbors, dim=0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233bfdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_labels = (torch.Tensor(assigned_target_labels) == torch.Tensor(usa_labels))\n",
    "matched_labels.sum()/len(usa_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfc176f",
   "metadata": {},
   "source": [
    "## Nearest Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286df31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "meta = json.load(open(\"..//metadata/geoplaces.json\"))\n",
    "\n",
    "# cap_src = \"llm_cap_llama_13b\"\n",
    "cap_src = \"caption\"\n",
    "\n",
    "## usa\n",
    "id_to_cap_usa = {e[\"image_id\"]:e[cap_src] for e in meta['usa_train']['metadata']}\n",
    "id_to_label_usa = {e[\"image_id\"]:e[\"category\"] for e in meta['usa_train']['annotations']}\n",
    "id_to_class_usa = {e[\"image_id\"]:e[\"class_name\"] for e in meta['usa_train']['annotations']}\n",
    "\n",
    "## asia\n",
    "id_to_cap_asia = {e[\"image_id\"]:e[cap_src] for e in meta['asia_train']['metadata']}\n",
    "id_to_label_asia = {e[\"image_id\"]:e[\"category\"] for e in meta['asia_train']['annotations']}\n",
    "id_to_class_asia = {e[\"image_id\"]:e[\"class_name\"] for e in meta['asia_train']['annotations']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e468ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_flickrids = list(id_to_cap_usa.keys())\n",
    "asia_flickrids = list(id_to_cap_asia.keys())\n",
    "\n",
    "usa_captions = [id_to_cap_usa[v] for v in usa_flickrids]\n",
    "usa_embeddings = model.encode(usa_captions)\n",
    "\n",
    "asia_captions = [id_to_cap_asia[v] for v in asia_flickrids]\n",
    "asia_embeddings = model.encode(asia_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352df8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_labels = torch.tensor([id_to_label_usa[v] for v in usa_flickrids])\n",
    "asia_labels = torch.tensor([id_to_label_asia[v] for v in asia_flickrids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cdeb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_classes = [id_to_class_usa[v] for v in usa_flickrids]\n",
    "asia_classes = [id_to_class_asia[v] for v in asia_flickrids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d21bb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = util.cos_sim(asia_embeddings, usa_embeddings)\n",
    "\n",
    "K = 5\n",
    "mostSimilar = similarity.topk(K, dim=-1).indices\n",
    "\n",
    "mostSimilar.shape\n",
    "\n",
    "mostSimilarLabels = usa_labels[mostSimilar]\n",
    "pseudoLabels = torch.mode(mostSimilarLabels, dim=-1).values\n",
    "\n",
    "matched_labels = (torch.Tensor(pseudoLabels) == torch.Tensor(asia_labels))\n",
    "matched_labels.sum()/len(asia_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27edf4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = util.cos_sim(usa_embeddings, asia_embeddings)\n",
    "\n",
    "K = 5\n",
    "mostSimilar = similarity.topk(K, dim=-1).indices\n",
    "\n",
    "mostSimilar.shape\n",
    "\n",
    "mostSimilarLabels = asia_labels[mostSimilar]\n",
    "pseudoLabels = torch.mode(mostSimilarLabels, dim=-1).values\n",
    "\n",
    "matched_labels = (torch.Tensor(pseudoLabels) == torch.Tensor(usa_labels))\n",
    "matched_labels.sum()/len(usa_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d7128a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_similarity_acc(asia_embeddings, usa_embeddings, asia_labels, usa_labels))\n",
    "print(get_similarity_acc(usa_embeddings, asia_embeddings, usa_labels, asia_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208be119",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_labels = (torch.Tensor(pseudoLabels) == torch.Tensor(asia_labels))\n",
    "matched_labels.sum()/len(asia_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67e0aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in np.random.choice(np.arange(1e5), 20):\n",
    "    index = int(idx)\n",
    "    otherIdx = mostSimilar[index][0]\n",
    "    print(\"[{}]{}:{}::{}:{}\\n\".format(int(asia_classes[index] == usa_classes[otherIdx]),\\\n",
    "                                      asia_captions[index],\\\n",
    "                               asia_classes[index],\\\n",
    "                               usa_captions[otherIdx],\n",
    "                               usa_classes[otherIdx]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e704921",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
