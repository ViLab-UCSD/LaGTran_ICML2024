{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe2bf34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "# import os\n",
    "import inflect\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "596d0c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pluralize(word):\n",
    "    p = inflect.engine()\n",
    "    return p.plural(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ff2c38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084f1d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_prefix(caption):\n",
    "    \n",
    "    if caption.startswith(\"An image of a \"):\n",
    "        caption = caption.split(\"An image of a \")[-1]\n",
    "    elif caption.startswith(\"An image of \"):\n",
    "        caption = caption.split(\"An image of \")[-1]\n",
    "    elif caption.startswith(\"An image of... \"):\n",
    "        caption = caption.split(\"An image of... \")[-1]\n",
    "        \n",
    "#     if caption.startswith(\"Sure\"):\n",
    "#         print(caption)\n",
    "#         print(\"&\"*100)\n",
    "\n",
    "    return caption\n",
    "\n",
    "def strip_classname(caption):\n",
    "    \n",
    "    if caption.startswith(\"Possible category: \"):\n",
    "        caption = caption.split(\"Possible category: \")[-1] \n",
    "    elif caption.startswith(\"Sure!\"):\n",
    "        caption = caption.rsplit(\"\\n\",1)[-1].strip()\n",
    "        \n",
    "    if \"\\n\" in caption:\n",
    "        caption = caption.rsplit(\"\\n\",1)[-1]\n",
    "        \n",
    "    return caption\n",
    "\n",
    "def get_classdesc(classnames, json_dict):\n",
    "    descriptions = []\n",
    "    for c in classnames:\n",
    "        name = c.replace(\"+\", \" \").replace(\"_\", \" \")\n",
    "        desc = \" or \".join(json_dict[c])\n",
    "        name_desc = \"{} with {}\".format(name, desc)\n",
    "        descriptions.append(name_desc)\n",
    "    return descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1cefec",
   "metadata": {},
   "source": [
    "### Caption - label similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c33b8a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(caption_embeddings, class_embeddings, topk=5):\n",
    "    similarity_matrix = util.cos_sim(caption_embeddings, class_embeddings)\n",
    "    pseudoLabel = similarity_matrix.topk(topk, 1).indices    \n",
    "    gt_label = torch.Tensor([id_to_classid[fid] for fid in all_flickrids]).view(-1,1).repeat(1,topk)\n",
    "    matched_labels = (torch.Tensor(pseudoLabel) == torch.Tensor(gt_label))\n",
    "    top1_acc = matched_labels[:,0].sum()/len(gt_label)\n",
    "    topk_acc = matched_labels.any(1).sum()/len(gt_label)\n",
    "    return top1_acc, topk_acc, similarity_matrix.argmax(1)\n",
    "\n",
    "# def get_confident(caption_embeddings, class_embeddings, thres=.0025):\n",
    "#     similarity_matrix = util.cos_sim(caption_embeddings, class_embeddings)\n",
    "#     similarity_matrix = torch.nn.functional.softmax(similarity_matrix, dim=-1)\n",
    "#     maxVal, pseudoLabel = similarity_matrix.max(1)\n",
    "#     valid_inds = maxVal > thres\n",
    "    \n",
    "    \n",
    "#     gt_label = torch.Tensor([id_to_classid[fid] for fid in all_flickrids])\n",
    "#     matched_labels = (torch.Tensor(pseudoLabel) == torch.Tensor(gt_label))\n",
    "#     matched_labels = matched_labels[valid_inds]\n",
    "#     top1_acc = matched_labels.sum()/len(valid_inds)\n",
    "# #     topk_acc = matched_labels.any(1).sum()/len(gt_label)\n",
    "#     return top1_acc, sum(valid_inds)/len(similarity_matrix)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2031dcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## category\n",
    "meta = json.load(open(\"../metadata/geoplaces.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b61b5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = {int(c[\"category_id\"]):c[\"category_name\"] for c in meta[\"categories\"]}\n",
    "# cats = [cats[idx].replace(\",\",\"\") for idx in range(68)]\n",
    "cats = [cats[idx].replace(\"_indoor\",\"\").replace(\"_outdoor\",\"\").replace(\"_\",\" \") for idx in range(205)]\n",
    "# cats = [cats[idx].replace(\"_\",\" \") for idx in range(205)]\n",
    "# cats = [cats[idx].replace(\"+\",\" \").replace(\"_\",\" \") for idx in range(600)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06972acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_embeddings = model.encode(cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec884997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usa:54.22/77.82\n",
      "asia:46.75/70.48\n"
     ]
    }
   ],
   "source": [
    "for src,dom in zip([\"asia\", \"usa\"], [\"usa\", \"asia\"]):\n",
    "    geo = meta['{}_train'.format(dom)]\n",
    "    id_to_cname = {ann[\"image_id\"]:ann[\"class_name\"] for ann in geo[\"annotations\"]}\n",
    "    id_to_classid = {ann[\"image_id\"]:ann[\"category\"] for ann in geo[\"annotations\"]}\n",
    "    id_to_blip = {m[\"image_id\"]:\" \".join([m[\"caption\"], m[\"tags\"].replace(\",\",\" \"), m[\"description\"]]) for m in geo[\"metadata\"]}\n",
    "#     id_to_blip = {m[\"image_id\"]:m[\"tags\"].replace(\",\",\" \") for m in geo[\"metadata\"]}\n",
    "#     id_to_cap = {e[\"image_id\"]:e[\"llm_cap_llama_13b\"] for e in geo[\"metadata\"]}\n",
    "\n",
    "#     cap_json = json.load(open(\"../metadata/condensed_caption_geoimnet_asia_13b.json\"))['captions']\n",
    "#     id_to_cap = {c[\"flickr_id\"]:c[\"condensed_caption\"] for c in cap_json}\n",
    "\n",
    "    all_flickrids = list(id_to_blip.keys())\n",
    "    all_captions = [id_to_blip[v] for v in all_flickrids]\n",
    "    caption_embeddings = model.encode(all_captions)\n",
    "\n",
    "    top1, top5, pseudo = get_accuracy(caption_embeddings, class_embeddings)\n",
    "    print(\"{}:{:.2f}/{:.2f}\".format(dom, top1*100, top5*100))\n",
    "    \n",
    "#     with open(\"../soft_labels/geoplaces_{}_{}_embedMatchPL.txt\".format(src, dom), \"w\") as fh:\n",
    "#         write_str = \"\"\n",
    "#         for fid, pl in zip(all_flickrids, pseudo):\n",
    "#             write_str += \"{} {}\\n\".format(fid, pl)\n",
    "#         fh.write(write_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714c9f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_captions[100:120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3499c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for idx, cname in id_to_cname.items():\n",
    "    tag_set = set(id_to_blip[idx].split(\" \"))\n",
    "    cls = set(cname.split(\",\"))\n",
    "    cls = set(map(lambda v:v.replace(\" \",\"\"), cls))\n",
    "    cls_aug = set(map(lambda v:pluralize(v), cls))\n",
    "    cls = cls.union(cls_aug)\n",
    "\n",
    "    common = tag_set.intersection(cls)\n",
    "    \n",
    "    if len(common) == 0:\n",
    "        print(\"Tags:{}\".format(tag_set))\n",
    "        print(\"Class:{}\".format(cls))\n",
    "        print(common)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615b661b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d40ffb1",
   "metadata": {},
   "source": [
    "### Caption - Caption similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af96aed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def get_similarity_acc(source_embed, target_embed, source_label, target_label, within=False, topks=[1,5]):\n",
    "    \n",
    "    topk = max(topks)\n",
    "        \n",
    "    if within:\n",
    "        similarity_matrix = util.cos_sim(source_embed, source_embed)\n",
    "        mostSimilar = similarity_matrix.topk(topk+1, 1).indices\n",
    "        mostSimilar = mostSimilar[:,1:]\n",
    "    else:\n",
    "        similarity_matrix = util.cos_sim(source_embed, target_embed)\n",
    "        mostSimilar = similarity_matrix.topk(topk, 1).indices\n",
    "    \n",
    "    similarLabels = torch.Tensor(target_label)[mostSimilar.long().reshape(-1)].reshape(-1, topk)\n",
    "    source_label = torch.Tensor(source_label).view(-1,1).repeat(1,topk)\n",
    "    \n",
    "    matched_labels = (torch.Tensor(similarLabels) == torch.Tensor(source_label))\n",
    "                      \n",
    "    top1_acc = matched_labels[:,0].sum()/len(source_label)\n",
    "    topk_acc = matched_labels.any(1).sum()/len(source_label)\n",
    "        \n",
    "    return top1_acc, topk_acc, similarLabels[:,0]\n",
    "\n",
    "def get_similarity_acc_knn(source_embed, target_embed, source_label, target_label, within=False, K=1):\n",
    "    \n",
    "    topk = K\n",
    "    \n",
    "    similarity_matrix = util.cos_sim(source_embed, target_embed)\n",
    "    mostSimilar = similarity_matrix.topk(topk, 1).indices\n",
    "    \n",
    "    similarLabels = torch.Tensor(target_label)[mostSimilar.long().reshape(-1)].reshape(-1, topk)\n",
    "    similarLabels = torch.mode(similarLabels, dim=-1, keepdim=True).values\n",
    "    source_label = torch.Tensor(source_label).view(-1,1)\n",
    "    \n",
    "    matched_labels = (torch.Tensor(similarLabels) == torch.Tensor(source_label))\n",
    "                      \n",
    "    top1_acc = matched_labels[:,0].sum()/len(source_label)\n",
    "#     topk_acc = matched_labels.any(1).sum()/len(source_label)\n",
    "        \n",
    "    return top1_acc, top1_acc, similarLabels[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f3e42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in ['geoplaces', 'geoimnet', 'geoyfcc']:\n",
    "    ## usa\n",
    "    for src, tgt in zip(['asia','usa'],['usa','asia']):\n",
    "        geo = json.load(open(f\"../metadata/{dataset}.json\"))\n",
    "        source = geo['{}_train'.format(src)]\n",
    "        id_to_cap_source = {m[\"image_id\"]:\" \".join([m[\"caption\"], m[\"tags\"].replace(\",\",\" \"), m[\"description\"]]) for m in source[\"metadata\"]}\n",
    "        id_to_label_source = {ann[\"image_id\"]:ann[\"category\"] for ann in source[\"annotations\"]}\n",
    "\n",
    "        ## asia\n",
    "        target = geo['{}_train'.format(tgt)]\n",
    "        id_to_cap_target = {m[\"image_id\"]:\" \".join([m[\"caption\"], m[\"tags\"].replace(\",\",\" \"), m[\"description\"]]) for m in target[\"metadata\"]}\n",
    "        id_to_label_target = {ann[\"image_id\"]:ann[\"category\"] for ann in target[\"annotations\"]}\n",
    "\n",
    "        source_flickrids = list(id_to_cap_source.keys())\n",
    "        target_flickrids = list(id_to_cap_target.keys())\n",
    "\n",
    "        source_captions = [id_to_cap_source[v] for v in source_flickrids]\n",
    "        source_embeddings = model.encode(source_captions)\n",
    "        source_labels = torch.tensor([id_to_label_source[v] for v in source_flickrids])\n",
    "\n",
    "        target_captions = [id_to_cap_target[v] for v in target_flickrids]\n",
    "        target_embeddings = model.encode(target_captions)\n",
    "        target_labels = torch.Tensor([id_to_label_target[v] for v in target_flickrids])\n",
    "\n",
    "        top1, top5, pseudo = get_similarity_acc_knn(target_embeddings, source_embeddings, target_labels, source_labels, K=3)\n",
    "\n",
    "        print(\"{}({}->{}):{:.2f}/{:.2f}\".format(dataset,src,tgt, top1*100, top5*100))\n",
    "        \n",
    "        assert len(target_flickrids) == len(pseudo)\n",
    "#         with open(\"../soft_labels/{}_{}_{}_knn3PL.txt\".format(dataset, src, tgt), \"w\") as fh:\n",
    "#             write_str = \"\"\n",
    "#             for fid, pl in zip(target_flickrids, pseudo):\n",
    "#                 write_str += \"{} {}\\n\".format(fid, pl)\n",
    "#             fh.write(write_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c11ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "geo = json.load(open(f\"../metadata/domainnet.json\"))\n",
    "source = geo['{}_train'.format(\"real\")]\n",
    "id_to_cap_source = {m[\"image_id\"]:m[\"blip2_cap\"] for m in source[\"metadata\"]}\n",
    "id_to_label_source = {ann[\"image_id\"]:ann[\"category\"] for ann in source[\"annotations\"]}\n",
    "\n",
    "for tgt in ['clipart', 'sketch', 'painting']:    \n",
    "\n",
    "    ## asia\n",
    "    target = geo['{}_train'.format(tgt)]\n",
    "    id_to_cap_target = {m[\"image_id\"]:m[\"blip2_cap\"] for m in target[\"metadata\"]}\n",
    "    id_to_label_target = {ann[\"image_id\"]:ann[\"category\"] for ann in target[\"annotations\"]}\n",
    "\n",
    "    source_flickrids = list(id_to_cap_source.keys())\n",
    "    target_flickrids = list(id_to_cap_target.keys())\n",
    "\n",
    "    source_captions = [id_to_cap_source[v] for v in source_flickrids]\n",
    "    source_embeddings = model.encode(source_captions)\n",
    "    source_labels = torch.tensor([id_to_label_source[v] for v in source_flickrids])\n",
    "\n",
    "    target_captions = [id_to_cap_target[v] for v in target_flickrids]\n",
    "    target_embeddings = model.encode(target_captions)\n",
    "    target_labels = torch.Tensor([id_to_label_target[v] for v in target_flickrids])\n",
    "\n",
    "    top1, top5, pseudo = get_similarity_acc_knn(target_embeddings, source_embeddings, target_labels, source_labels, K=201)\n",
    "\n",
    "    print(\"({}->{}):{:.2f}/{:.2f}\".format(\"real\",tgt, top1*100, top5*100))\n",
    "\n",
    "    assert len(target_flickrids) == len(pseudo)\n",
    "#     with open(\"../soft_labels/{}_{}_{}_knn3PL.txt\".format(\"domainnet\", \"real\", tgt), \"w\") as fh:\n",
    "#         write_str = \"\"\n",
    "#         for fid, pl in zip(target_flickrids, pseudo):\n",
    "#             write_str += \"{} {}\\n\".format(fid, pl)\n",
    "#         fh.write(write_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5bd444",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_similarity_acc(usa_embeddings, asia_embeddings, usa_labels, asia_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27abdd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_similarity_acc(asia_embeddings, usa_embeddings, asia_labels, usa_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ff47c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix = util.cos_sim(asia_embeddings, usa_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cb9000",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = torch.sort(similarity_matrix, descending=True, dim=0).indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912a3688",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_orderedNeighbors = torch.Tensor(asia_labels)[ind[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba38abbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_orderedNeighbors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76f6c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "assigned_target_labels = torch.mode(k_orderedNeighbors, dim=0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233bfdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_labels = (torch.Tensor(assigned_target_labels) == torch.Tensor(usa_labels))\n",
    "matched_labels.sum()/len(usa_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfc176f",
   "metadata": {},
   "source": [
    "## Nearest Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286df31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "meta = json.load(open(\"..//metadata/geoplaces.json\"))\n",
    "\n",
    "# cap_src = \"llm_cap_llama_13b\"\n",
    "cap_src = \"caption\"\n",
    "\n",
    "## usa\n",
    "id_to_cap_usa = {e[\"image_id\"]:e[cap_src] for e in meta['usa_train']['metadata']}\n",
    "id_to_label_usa = {e[\"image_id\"]:e[\"category\"] for e in meta['usa_train']['annotations']}\n",
    "id_to_class_usa = {e[\"image_id\"]:e[\"class_name\"] for e in meta['usa_train']['annotations']}\n",
    "\n",
    "## asia\n",
    "id_to_cap_asia = {e[\"image_id\"]:e[cap_src] for e in meta['asia_train']['metadata']}\n",
    "id_to_label_asia = {e[\"image_id\"]:e[\"category\"] for e in meta['asia_train']['annotations']}\n",
    "id_to_class_asia = {e[\"image_id\"]:e[\"class_name\"] for e in meta['asia_train']['annotations']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e468ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_flickrids = list(id_to_cap_usa.keys())\n",
    "asia_flickrids = list(id_to_cap_asia.keys())\n",
    "\n",
    "usa_captions = [id_to_cap_usa[v] for v in usa_flickrids]\n",
    "usa_embeddings = model.encode(usa_captions)\n",
    "\n",
    "asia_captions = [id_to_cap_asia[v] for v in asia_flickrids]\n",
    "asia_embeddings = model.encode(asia_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352df8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_labels = torch.tensor([id_to_label_usa[v] for v in usa_flickrids])\n",
    "asia_labels = torch.tensor([id_to_label_asia[v] for v in asia_flickrids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cdeb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_classes = [id_to_class_usa[v] for v in usa_flickrids]\n",
    "asia_classes = [id_to_class_asia[v] for v in asia_flickrids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d21bb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = util.cos_sim(asia_embeddings, usa_embeddings)\n",
    "\n",
    "K = 5\n",
    "mostSimilar = similarity.topk(K, dim=-1).indices\n",
    "\n",
    "mostSimilar.shape\n",
    "\n",
    "mostSimilarLabels = usa_labels[mostSimilar]\n",
    "pseudoLabels = torch.mode(mostSimilarLabels, dim=-1).values\n",
    "\n",
    "matched_labels = (torch.Tensor(pseudoLabels) == torch.Tensor(asia_labels))\n",
    "matched_labels.sum()/len(asia_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27edf4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = util.cos_sim(usa_embeddings, asia_embeddings)\n",
    "\n",
    "K = 5\n",
    "mostSimilar = similarity.topk(K, dim=-1).indices\n",
    "\n",
    "mostSimilar.shape\n",
    "\n",
    "mostSimilarLabels = asia_labels[mostSimilar]\n",
    "pseudoLabels = torch.mode(mostSimilarLabels, dim=-1).values\n",
    "\n",
    "matched_labels = (torch.Tensor(pseudoLabels) == torch.Tensor(usa_labels))\n",
    "matched_labels.sum()/len(usa_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d7128a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_similarity_acc(asia_embeddings, usa_embeddings, asia_labels, usa_labels))\n",
    "print(get_similarity_acc(usa_embeddings, asia_embeddings, usa_labels, asia_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208be119",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_labels = (torch.Tensor(pseudoLabels) == torch.Tensor(asia_labels))\n",
    "matched_labels.sum()/len(asia_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67e0aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in np.random.choice(np.arange(1e5), 20):\n",
    "    index = int(idx)\n",
    "    otherIdx = mostSimilar[index][0]\n",
    "    print(\"[{}]{}:{}::{}:{}\\n\".format(int(asia_classes[index] == usa_classes[otherIdx]),\\\n",
    "                                      asia_captions[index],\\\n",
    "                               asia_classes[index],\\\n",
    "                               usa_captions[otherIdx],\n",
    "                               usa_classes[otherIdx]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e704921",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
