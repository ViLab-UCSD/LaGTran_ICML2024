{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35d49d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "from itertools import chain\n",
    "# import os\n",
    "%config Completer.use_jedi = False\n",
    "\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c0ca0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c57ad3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngrams(text, n, sep=\" \"):\n",
    "    \"\"\"\n",
    "    Function to generate all ngrams for a given 'n' from a string.\n",
    "\n",
    "    :param text: The input string from which to generate ngrams.\n",
    "    :param n: The size of the ngram.\n",
    "    :return: A list of ngrams as strings.\n",
    "    \"\"\"\n",
    "    if isinstance(n, list):\n",
    "        ngram_list = []\n",
    "        for ni in n:\n",
    "            ngram_list += generate_ngrams(text, ni, sep)\n",
    "        return ngram_list\n",
    "    # Split the text into words\n",
    "    words = text.split(sep)\n",
    "\n",
    "    # Generate ngrams\n",
    "    ngram_list = [' '.join(gram) for gram in ngrams(words, n)]\n",
    "\n",
    "    return ngram_list\n",
    "\n",
    "def preprocess(text):\n",
    "    caption = text[\"caption\"]\n",
    "    ngram_caption = generate_ngrams(caption, [1,2,3,4])\n",
    "    \n",
    "    tags = text[\"tags\"]#.split(\",\")\n",
    "    ngram_tags = generate_ngrams(tags, [1,2,3,4], sep=\",\")\n",
    "#     ngram_tags = tags\n",
    "    \n",
    "    all_ngrams = ngram_caption + ngram_tags\n",
    "    return list(set(all_ngrams))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7c3c3c",
   "metadata": {},
   "source": [
    "## Places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff577c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_DESC_LEN=12\n",
    "N_CLS=205"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb334728",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_name(cname):\n",
    "    return cname.replace(\"_indoor\",\"\").replace(\"_outdoor\",\"\").replace(\"_\",\" \").replace(\"-\",\" \")\n",
    "\n",
    "def get_accuracy(similarity_matrix, topk=5):\n",
    "    pseudoLabel = similarity_matrix.topk(topk, 1).indices  \n",
    "    gt_label = torch.Tensor([id_to_classid[fid] for fid in all_flickrids]).view(-1,1).repeat(1,topk)\n",
    "    matched_labels = (torch.Tensor(pseudoLabel) == torch.Tensor(gt_label))\n",
    "    top1_acc = matched_labels[:,0].sum()/len(gt_label)\n",
    "    topk_acc = matched_labels.any(1).sum()/len(gt_label)\n",
    "    return top1_acc, topk_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1609b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "places_desc = json.load(open(\"descriptors_geoplaces.json\"))\n",
    "places_desc = [[strip_name(k)] + v for k,v in places_desc.items()]\n",
    "# places_desc = [[strip_name(k)] for k,v in places_desc.items()]\n",
    "\n",
    "padded_desc = []\n",
    "mask = []\n",
    "for p in places_desc:\n",
    "    pad_len = MAX_DESC_LEN-len(p)\n",
    "    padded_desc.append(p + ['EOS']*pad_len)\n",
    "    mask.append([1]*len(p) + [0]*pad_len)\n",
    "mask = torch.Tensor(mask)\n",
    "mask = mask[None]\n",
    "\n",
    "all_desc = list(chain.from_iterable(padded_desc))\n",
    "\n",
    "all_embeddings = model.encode(all_desc)\n",
    "\n",
    "## load data and json files\n",
    "domain = \"asia\"\n",
    "meta = json.load(open(\"/home/tarun/metadata/geoPlaces_metadata.json\"))\n",
    "geo = meta[f'{domain}_train']\n",
    "id_to_classid = {ann[\"image_id\"]:ann[\"category\"] for ann in geo[\"annotations\"]}\n",
    "\n",
    "cap = json.load(open(f\"/home/tarun/llama/extracted_captions_geoplaces_{domain}.json\"))[\"extracted\"]\n",
    "id_to_cap = {e[\"flickr_id\"]:e[\"extracted_class_name\"] for e in cap}\n",
    "\n",
    "all_flickrids = list(id_to_cap.keys())\n",
    "all_captions = [id_to_cap[v] for v in all_flickrids]\n",
    "caption_embeddings = model.encode(all_captions)\n",
    "\n",
    "similarity_matrix = util.cos_sim(caption_embeddings, all_embeddings)\n",
    "similarity_matrix = similarity_matrix.reshape(-1, N_CLS, MAX_DESC_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2545c0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix = similarity_matrix * mask\n",
    "\n",
    "averaged_similarity = similarity_matrix.max(-1).values#/mask.sum(-1)\n",
    "\n",
    "get_accuracy(averaged_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c3083c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_captions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1480c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "places_desc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e537e0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda101b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask.sum(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37d6c54",
   "metadata": {},
   "source": [
    "## GeoImnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a12dd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_DESC_LEN=12\n",
    "N_CLS=600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72cf739",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_name(cname):\n",
    "    return cname.replace(\"+\",\" \").replace(\"_\",\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a181d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "imnet_desc = json.load(open(\"descriptors_geoimnet.json\"))\n",
    "imnet_desc = [[strip_name(k)] + v for k,v in imnet_desc.items()]\n",
    "# imnet_desc = [[strip_name(k)] for k,v in imnet_desc.items()]\n",
    "\n",
    "padded_desc = []\n",
    "mask = []\n",
    "for p in imnet_desc:\n",
    "    pad_len = MAX_DESC_LEN-len(p)\n",
    "    padded_desc.append(p + ['EOS']*pad_len)\n",
    "    mask.append([1]*len(p) + [0]*pad_len)\n",
    "mask = torch.Tensor(mask)\n",
    "mask = mask[None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f03f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_desc = list(chain.from_iterable(padded_desc))\n",
    "\n",
    "all_embeddings = model.encode(all_desc)\n",
    "\n",
    "## load data and json files\n",
    "domain = \"asia\"\n",
    "meta = json.load(open(\"/home/tarun/metadata/geoImnet_metadata.json\"))\n",
    "geo = meta[f'{domain}_train']\n",
    "id_to_classid = {ann[\"image_id\"]:ann[\"category\"] for ann in geo[\"annotations\"]}\n",
    "\n",
    "cap = json.load(open(f\"/home/tarun/llama/extracted_captions_geoimnet_{domain}.json\"))[\"extracted\"]\n",
    "id_to_cap = {e[\"flickr_id\"]:e[\"extracted_class_name\"] for e in cap}\n",
    "\n",
    "all_flickrids = list(id_to_cap.keys())\n",
    "all_captions = [id_to_cap[v] for v in all_flickrids]\n",
    "caption_embeddings = model.encode(all_captions)\n",
    "\n",
    "similarity_matrix = util.cos_sim(caption_embeddings, all_embeddings)\n",
    "similarity_matrix = similarity_matrix.reshape(-1, N_CLS, MAX_DESC_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8898a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix = similarity_matrix * mask\n",
    "\n",
    "averaged_similarity = similarity_matrix.sum(-1)/mask.sum(-1)\n",
    "\n",
    "get_accuracy(averaged_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c437031c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask.sum(-1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80ec91c",
   "metadata": {},
   "source": [
    "## Use tags to find the best possible labels: GeoImnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a531b911",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(similarity_matrix, gt_label, topk=5):\n",
    "    pseudoLabel = similarity_matrix.topk(topk, 1).indices  \n",
    "    gt_label = torch.Tensor(gt_label).view(-1,1).repeat(1,topk)\n",
    "    matched_labels = (torch.Tensor(pseudoLabel) == torch.Tensor(gt_label))\n",
    "    top1_acc = matched_labels[:,0].sum()/len(gt_label)\n",
    "    topk_acc = matched_labels.any(1).sum()/len(gt_label)\n",
    "    return top1_acc, topk_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0d9c267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: geoyfcc, asia->usa\n",
      "(tensor(0.9276), tensor(0.9963))\n",
      "Name: geoyfcc, usa->asia\n",
      "(tensor(0.9197), tensor(0.9947))\n"
     ]
    }
   ],
   "source": [
    "# for name in [\"GeoPlaces\", \"GeoImnet\", \"GeoYFCC\"]:\n",
    "for name in [\"GeoYFCC\"]:\n",
    "    for source, target in zip([\"asia\", \"usa\"],[\"usa\",\"asia\"]):\n",
    "        data = json.load(open(\"../metadata/{}.json\".format(name.lower())))\n",
    "        geo = data[f'{target}_train']\n",
    "        id_to_classid = {ann[\"image_id\"]:ann[\"category\"] for ann in geo[\"annotations\"]}\n",
    "        \n",
    "        if name == \"GeoImnet\":\n",
    "            classnames = [v['category_name'].replace(\"+\",\" \").replace(\"_\",\" \") for v in data['categories']]\n",
    "        elif name == \"GeoPlaces\":\n",
    "            classnames = [v['category_name'].replace(\"_indoor\",\"\").replace(\"_outdoor\",\"\").replace(\"_\",\" \") for v in data['categories']]\n",
    "        else:\n",
    "            classnames = [v['category_name'].replace(\",\",\"\") for v in data['categories']]\n",
    "            \n",
    "        class_embeddings = model.encode(classnames)\n",
    "\n",
    "        id_to_tags = {m[\"image_id\"]: preprocess(m) for m in geo[\"metadata\"]}\n",
    "        all_flickrids = list(id_to_tags.keys())\n",
    "        all_tags = [id_to_tags[f] for f in all_flickrids]\n",
    "        tag_lens = [len(t) for t in all_tags]\n",
    "        MAX_TAG_LEN=90#max(tag_lens)\n",
    "\n",
    "        padded_tags = []\n",
    "        mask = []\n",
    "        for p in all_tags:\n",
    "            pad_len = max(0, MAX_TAG_LEN-len(p))\n",
    "            padded_tags.append(p[:MAX_TAG_LEN] + ['EOS']*pad_len)\n",
    "            mask.append([1]*min(MAX_TAG_LEN, len(p)) + [0]*pad_len)\n",
    "        mask = torch.Tensor(mask)\n",
    "\n",
    "        flattened_tags = list(chain.from_iterable(padded_tags))\n",
    "\n",
    "        tag_embedding = model.encode(flattened_tags, batch_size=256)\n",
    "\n",
    "        similarity = util.cos_sim(tag_embedding, class_embeddings)\n",
    "\n",
    "        similarity_reshaped = similarity.reshape(*mask.shape, -1)\n",
    "\n",
    "        mask = mask[...,None]\n",
    "\n",
    "        masked_similarity = similarity_reshaped * mask\n",
    "\n",
    "        pseudo_labels = masked_similarity.max(1).values\n",
    "\n",
    "        gt_label = [id_to_classid[f] for f in all_flickrids]\n",
    "        \n",
    "        print(\"Name: {}, {}->{}\".format(name.lower(), source, target))\n",
    "        print(get_accuracy(pseudo_labels, gt_label))\n",
    "        \n",
    "        tag_label = pseudo_labels.argmax(1).cpu().numpy()\n",
    "        write_str = \"\"\n",
    "#         with open(\"../soft_labels/{}_{}_{}_tagMatchPL.txt\".format(name.lower(), source, target), \"w\") as fh:\n",
    "#             for pl, fid in tqdm(zip(tag_label, all_flickrids)):\n",
    "#                 write_str += f\"{fid} \"\n",
    "#                 write_str += f\"{pl}\"\n",
    "#                 write_str += \"\\n\"\n",
    "#             fh.write(write_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d74068",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(id_to_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e33cbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_flickrids = list(id_to_tags.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe11c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "flickrid = all_flickrids[random.sample(range(len(all_flickrids)) ,1)[0]]\n",
    "print(flickrid)\n",
    "print(id_to_tags[flickrid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa689392",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(id_to_tags[3560730811])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e1fa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_metadata = {m[\"image_id\"]: m for m in geo[\"metadata\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb313412",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_metadata[3560730811]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cd879e",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_ngrams(id_to_metadata[3560730811][\"caption\"], [1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac3d640",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb0096a",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_tags[14031943238]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db8e037",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d89e3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_tag_label = torch.nn.functional.softmax(pseudo_labels/0.07, dim=-1).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aab22aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_str = \"\"\n",
    "with open(\"../soft_labels/{}_{}_tagPL.txt\".format(name, domain), \"w\") as fh:\n",
    "    for pl, fid in tqdm(zip(soft_tag_label, all_flickrids)):\n",
    "        write_str += f\"{fid} \"\n",
    "        vals = list(map(str, pl.tolist()))\n",
    "        write_str += \" \".join(vals)\n",
    "        write_str += \"\\n\"\n",
    "    fh.write(write_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60135b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "name, domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d4e5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(tag_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b72799",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
