{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c6cb62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "843c5f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"officeHome\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3cdb590",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_original = json.load(open(\"../metadata/{}.json\".format(dataset.lower())))\n",
    "# metadata_blip = json.load(open(\"/newdata/tarun/datasets/WebVision/geoImnet.json\".format(dataset)))\n",
    "# metadata_blip = json.load(open(\"/newfoundland2/tarun/datasets/Places205/data/vision/torralba/deeplearning/GeoDA/geo{}.json\".format(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eee17a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dom in ['art', 'product', 'real', 'clipart']:\n",
    "    \n",
    "#     id_to_blip1 = {e[\"image_id\"]:e[\"blip_cap\"] for e in metadata_blip['{}_train'.format(dom)]['metadata']}\n",
    "    blip2_metadata = json.load(open(\"../metadata/{}/blip2_captions_{}_{}_train.json\".format(dataset, dataset.lower(), dom)))['captions']\n",
    "    id_to_blip2 = {e[\"flickr_id\"]:e[\"blip_caption\"] for e in blip2_metadata}\n",
    "    \n",
    "#     llm_caps = json.load(open(\"../metadata/caponly_condensed_caption_geo{}_{}_13b.json\".format(dataset.lower(),dom)))[\"captions\"]\n",
    "#     id_to_llmcap = {e[\"flickr_id\"]:e[\"condensed_caption\"] for e in llm_caps}\n",
    "    \n",
    "    dom_metadata = metadata_original['{}_train'.format(dom)]\n",
    "    new_metadata = []\n",
    "    for m in dom_metadata['metadata']:\n",
    "        m.update({\n",
    "#                 \"llm_cap_llama_13b\" : id_to_llmcap[m[\"image_id\"]],\n",
    "#                 \"blip1_cap\"         : id_to_blip1[m[\"image_id\"]],\n",
    "                \"blip2_cap\"         : id_to_blip2[m[\"image_id\"]]\n",
    "            })\n",
    "        new_metadata.append(m)\n",
    "    metadata_original['{}_train'.format(dom)]['metadata'] = new_metadata\n",
    "    metadata_original['{}_test'.format(dom)]['metadata'] = new_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82c7ac01",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../metadata/officehome.json\".format(dataset.lower()), \"w\") as fh:\n",
    "    json.dump(metadata_original, fh, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ba6bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dom in ['real', \"clipart\", \"sketch\", \"painting\", \"quickdraw\", \"infograph\"]:\n",
    "    \n",
    "    metadata_blip = json.load(open(\"../metadata/domainnet_blip/blip2_captions_domainnet_{}.json\".format(dom)))['captions']\n",
    "    id_to_blip2 = {e[\"flickr_id\"]:e[\"blip_caption\"] for e in metadata_blip}\n",
    "    \n",
    "    \n",
    "    dom_metadata = metadata_original['{}_train'.format(dom)]\n",
    "    new_metadata = []\n",
    "    for m in dom_metadata['metadata']:\n",
    "        m.update({\n",
    "#                 \"llm_cap_llama_13b\" : id_to_llmcap[m[\"image_id\"]],\n",
    "#                 \"blip1_cap\"         : id_to_blip1[m[\"image_id\"]],\n",
    "                \"blip2_cap\"         : id_to_blip2.get(m[\"image_id\"]),\n",
    "            })\n",
    "        new_metadata.append(m)\n",
    "    metadata_original['{}_train'.format(dom)]['metadata'] = new_metadata\n",
    "    \n",
    "with open(\"../metadata/{}.json\".format(dataset.lower()), \"w\") as fh:\n",
    "    json.dump(metadata_original, fh, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f18eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_original['real_train']['metadata'][10300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c87729f",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_original = json.load(open(\"/home/tarun/metadata/geoUniDA_metadata.json\"))\n",
    "\n",
    "for dom in ['asia', 'usa']:\n",
    "    \n",
    "    llm_caps = json.load(open(\"../metadata/condensed_caption_geo{}_{}_13b.json\".format(dataset.lower(),dom)))[\"captions\"]\n",
    "    id_to_llmcap = {e[\"flickr_id\"]:e[\"condensed_caption\"] for e in llm_caps}\n",
    "    \n",
    "    dom_metadata = metadata_original['{}'.format(dom)]\n",
    "    new_metadata = []\n",
    "    for m in dom_metadata['metadata']:\n",
    "        m.update({\n",
    "                \"llm_cap_llama_13b\" : id_to_llmcap[m[\"image_id\"]],\n",
    "#                 \"blip1_cap\"         : id_to_blip1[m[\"image_id\"]],\n",
    "#                 \"blip2_cap\"         : id_to_blip2.get(m[\"image_id\"], id_to_blip1[m[\"image_id\"]])\n",
    "            })\n",
    "        new_metadata.append(m)\n",
    "    metadata_original['{}'.format(dom)]['metadata'] = new_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fca9c2",
   "metadata": {},
   "source": [
    "## GeoYFCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14e79ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_original = json.load(open(\"/home/tarun/metadata/geoyfcc.json\"))\n",
    "for dom in [\"usa\", \"asia\"]:\n",
    "\n",
    "    metadata_blip = json.load(open(\"../metadata/blip2_captions_geoyfcc_{}.json\".format(dom)))['captions']\n",
    "    id_to_blip = {m[\"flickr_id\"]:m[\"blip_caption\"] for m in metadata_blip}\n",
    "\n",
    "    metadata_llm = json.load(open(\"../metadata/condensed_caption_geoyfcc_{}_13b.json\".format(dom)))['captions']\n",
    "    id_to_llm = {m[\"flickr_id\"]:m[\"condensed_caption\"] for m in metadata_llm}\n",
    "\n",
    "    dom_metadata = metadata_original['{}_train'.format(dom)]\n",
    "    new_metadata = []\n",
    "    for m in dom_metadata['metadata']:\n",
    "        m.update({\n",
    "                \"llm_cap_llama_13b\" : id_to_llm[m[\"image_id\"]],\n",
    "                \"blip1_cap\"         : id_to_blip[m[\"image_id\"]],\n",
    "                \"blip2_cap\"         : id_to_blip[m[\"image_id\"]]\n",
    "            })\n",
    "        new_metadata.append(m)\n",
    "    metadata_original['{}_train'.format(dom)]['metadata'] = new_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba58dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../metadata/geoyfcc.json\", \"w\") as fh:\n",
    "    json.dump(metadata_original, fh, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fc6c0d",
   "metadata": {},
   "source": [
    "## Visualize captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73443980",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7389ff7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_img(image):\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3ae251",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open(\"../metadata/domainnet.json\"))['clipart_train']\n",
    "root_dir = \"/newfoundland/tarun/datasets/Adaptation/visDA/\"\n",
    "\n",
    "# data = json.load(open(\"../metadata/geoplaces.json\"))['asia_train']\n",
    "# root_dir = \"/newdata/tarun/datasets/GeoNet/metadata/GeoPlaces/\"\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae5a3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_meta = {e[\"image_id\"]:e for e in data['metadata']}\n",
    "id_to_class = {e[\"image_id\"]:e[\"class_name\"] for e in data['annotations']}\n",
    "id_to_file = {e[\"id\"]:e[\"filename\"] for e in data['images']}\n",
    "all_ids = list(id_to_meta.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e594ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = json.load(open(\"../metadata/domainnet.json\"))['categories']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fa1b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "cid_to_name = {c['category_id'] : c['category_name'] for c in cats}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd4fcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ids = [\n",
    "    6847723442,\n",
    "8515982584,\n",
    "7112161493,\n",
    "5289383847,\n",
    "4935275204,\n",
    "4071563477,\n",
    "149224226,\n",
    "8396995227,\n",
    "5961282862,\n",
    "2724789996,\n",
    "6285150681,\n",
    "6285144895]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2621dd26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for idxs in random.sample(all_ids, 20):\n",
    "#     if \"courthouse\" not in id_to_class[idxs]:\n",
    "#         continue\n",
    "#     tags = id_to_meta[idxs][\"caption\"]\n",
    "#     if tags != \"\":\n",
    "#         continue\n",
    "    \n",
    "    img = np.asarray(Image.open(os.path.join(root_dir, id_to_file[idxs])))\n",
    "    show_img(img)\n",
    "#     print(\"Tags:{}\".format(id_to_meta[idxs][\"tags\"]))\n",
    "#     print(\"Title:{}\".format(id_to_meta[idxs][\"caption\"]))\n",
    "#     print(\"Description:{}\".format(id_to_meta[idxs][\"description\"]))\n",
    "#     print(\"LLM Cap:{}\".format(id_to_meta[idxs][\"llm_cap_llama_13b\"]))\n",
    "    print(\"BLIP Cap:{}\".format(id_to_meta[idxs][\"blip2_cap\"]))\n",
    "#     print(\"Label:{}({})\".format(id_to_class[idxs], idxs))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d0d70c",
   "metadata": {},
   "source": [
    "## PL Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb88a0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "085a3bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "domains = [\"asia\", \"usa\"]\n",
    "dnet_data = json.load(open(\"../metadata/geoplaces.json\"))\n",
    "\n",
    "accs = []\n",
    "\n",
    "for src in domains:\n",
    "    for tgt in domains:\n",
    "        if src == tgt:\n",
    "            continue\n",
    "        pseudo = pd.read_csv(f\"../hard_labels/geoplaces_{src}_{tgt}_combinedPL.txt\", sep=\"\\s\", engine=\"python\", header=None)\n",
    "        flickr_ids = list(map(int, pseudo.iloc[:,0].values))\n",
    "        labels = torch.Tensor(np.array(pseudo.iloc[:,1:].values)).reshape(-1)\n",
    "        \n",
    "        data = dnet_data[f'{tgt}_train']\n",
    "        id_to_class = {e[\"image_id\"]:e[\"category\"] for e in data['annotations']}\n",
    "        gt_labels = torch.tensor([id_to_class[fid] for fid in flickr_ids], dtype=torch.long).reshape(-1)\n",
    "        \n",
    "        accs.append(torch.sum(gt_labels == labels)/len(labels))\n",
    "# print(torch.mean(torch.tensor(accs)).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e1f12af1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.7324), tensor(0.6437)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63f36e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7146511673927307\n"
     ]
    }
   ],
   "source": [
    "domains = [\"real\", \"clipart\", \"sketch\", \"painting\"]\n",
    "dnet_data = json.load(open(\"../metadata/domainnet.json\"))\n",
    "\n",
    "accs = []\n",
    "\n",
    "for src in domains:\n",
    "    for tgt in domains:\n",
    "        if src == tgt:\n",
    "            continue\n",
    "        pseudo = pd.read_csv(f\"../hard_labels/domainnet_{src}_{tgt}_blipPL.txt\", sep=\"\\s\", engine=\"python\", header=None)\n",
    "        flickr_ids = list(map(int, pseudo.iloc[:,0].values))\n",
    "        labels = torch.Tensor(np.array(pseudo.iloc[:,1:].values)).reshape(-1)\n",
    "        \n",
    "        data = dnet_data[f'{tgt}_train']\n",
    "        id_to_class = {e[\"image_id\"]:e[\"category\"] for e in data['annotations']}\n",
    "        gt_labels = torch.tensor([id_to_class[fid] for fid in flickr_ids], dtype=torch.long).reshape(-1)\n",
    "        \n",
    "        accs.append(torch.sum(gt_labels == labels)/len(labels))\n",
    "print(torch.mean(torch.tensor(accs)).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e622d930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6321)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99afbb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo = pd.read_csv(\"../hard_labels/visda2017_syn_real_blipPL.txt\", sep=\"\\s\", engine=\"python\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e4ad3e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = dict(zip(pseudo.iloc[:,0], pseudo.iloc[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "540f20d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "flickr_ids = list(map(int, pseudo.iloc[:,0].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9245a8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.Tensor(np.array(pseudo.iloc[:,1:].values)).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c20c62d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open(\"../metadata/visda2017.json\"))['real_train']\n",
    "id_to_class = {e[\"image_id\"]:e[\"category\"] for e in data['annotations']}\n",
    "gt_labels = torch.tensor([id_to_class[fid] for fid in flickr_ids], dtype=torch.long).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a039523b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open(\"../metadata/visda2017.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c1809ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44305"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data['real_train'][\"images\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f37aa77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6967)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(gt_labels == labels)/len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b0f262",
   "metadata": {},
   "outputs": [],
   "source": [
    "## collect all errors\n",
    "error_ids = []\n",
    "for fid, pred, gt in zip(flickr_ids, labels.tolist(), gt_labels.tolist()):\n",
    "    if pred != gt:\n",
    "        error_ids.append(fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ed499b",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_class = {e[\"image_id\"]:e[\"class_name\"] for e in data['annotations']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b9b025",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for idxs in random.sample(error_ids, 20):\n",
    "#     if \"courthouse\" not in id_to_class[idxs]:\n",
    "#         continue\n",
    "#     tags = id_to_meta[idxs][\"caption\"]\n",
    "#     if tags != \"\":\n",
    "#         continue\n",
    "    \n",
    "    img = np.asarray(Image.open(os.path.join(root_dir, id_to_file[idxs])))\n",
    "    show_img(img)\n",
    "#     print(\"Tags:{}\".format(id_to_meta[idxs][\"tags\"]))\n",
    "#     print(\"Title:{}\".format(id_to_meta[idxs][\"caption\"]))\n",
    "#     print(\"Description:{}\".format(id_to_meta[idxs][\"description\"]))\n",
    "#     print(\"LLM Cap:{}\".format(id_to_meta[idxs][\"llm_cap_llama_13b\"]))\n",
    "    print(\"BLIP Cap:{}\".format(id_to_meta[idxs][\"blip2_cap\"]))\n",
    "    print(\"GT Label:{}\".format(id_to_class[idxs]))\n",
    "    print(\"Pred Label:{}\".format(cid_to_name[pred_labels[idxs]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a780f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "geop = json.load(open(\"../metadata/geoplaces.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52f7364",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in geop['usa_test']['metadata']:\n",
    "    if m[\"image_id\"] == 4505175031:\n",
    "        print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a37ef3",
   "metadata": {},
   "source": [
    "## visDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6f46ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b32d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = 0\n",
    "for c in visda[\"syn_train\"][\"metadata\"]:\n",
    "    if \"3d\" in c[\"blip2_cap\"].lower():\n",
    "        counts += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c1483fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visda = json.load(open(\"../metadata/visda2017/blip2_captions_visda2017_syn_train.json\"))['captions']\n",
    "visda = json.load(open(\"../metadata/visda2017.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61c67746",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_visda(cap):\n",
    "    \n",
    "    tokens = [\"a 3d model of a \", \"3d model of a \", \"3d model of \", \"3d model \", \"on a white background \", \"of a white background \", \"on a white surface \", \"of a white surface \", \"a white background \", \"a white surface \", \"white background \", \"white surface \", \"white \", \"background \", \"surface \", \"is shown in a 3d rendering \", \"in a 3d rendering \", \"3d rendering \"]\n",
    "    \n",
    "    for t in tokens:\n",
    "        cap = cap.replace(t,\"\")\n",
    "        cap = cap.replace(t[:-1],\"\")\n",
    "        \n",
    "    return cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8d5450a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_list = []\n",
    "for meta in visda[\"syn_train\"][\"metadata\"]:\n",
    "    meta[\"blip2_cap\"] = filter_visda(meta[\"blip2_cap\"])\n",
    "    new_list.append(meta)\n",
    "\n",
    "visda[\"syn_train\"][\"metadata\"] = new_list\n",
    "visda[\"syn_test\"][\"metadata\"] = new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82dcf11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../metadata/visda2017.json\", \"w\") as fh:\n",
    "    json.dump(visda, fh, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "514046b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "visda = json.load(open(\"../metadata/visda2017.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2014128c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6871806321501 a living room with a fireplace, a piano and a couch\n",
      "187226018106912 a small tree in a planter on a grassy field\n",
      "150443372824736 a bed with a white sheet and a vase of flowers\n",
      "190389829592361 a man in a white shirt and blue tie is standing next to a plant\n",
      "198261695753373 a living room with a large bookcase and a large potted plant\n",
      "176977201878034 a wooden barrel with flowers in it sitting on a stone patio\n",
      "275823168192705 a vase with flowers on a table next to a gray couch\n",
      "91506166507971 a person holding a plant in front of a plant stand\n",
      "30168305781053 a vase with flowers in it\n",
      "278458367415858 the patio at the inn at the top of the hill\n",
      "120622795083794 a room with a table with a vase of yellow flowers and pictures on the wall\n",
      "247704002297679 a fish tank with plants and a fish in it\n",
      "240382779212633 a street sign with a red flower in the middle of it\n",
      "210035750581751 a bathroom with a large mirror and a sink\n",
      "162027346499172 a kitchen counter with silverware and flowers\n",
      "105545484832570 a living room with a table and a potted plant\n",
      "147792184497509 a living room with a large potted plant on the floor\n",
      "91115312063392 a man standing in front of a screen showing a game\n",
      "147003726171182 a bathroom with a sink, mirror and flowers in the vase\n",
      "212575214631669 a kitchen with a refrigerator and a stove\n",
      "247850963660300 a kitchen counter with a bowl of oranges and a vase of poinsettia\n",
      "242560813010065 a crow and a dog in a garden\n",
      "81298226180920 a woman in a red coat is holding a flower\n",
      "256925446059991 a man standing in front of a fireplace with a vase of flowers\n",
      "170375149151383 a man walking in the rain with a umbrella\n",
      "213634626500507 a statue of a woman with flowers in her hair\n",
      "37588445103535 a vase with flowers in it\n",
      "280598084449178 a flower garden with purple flowers on a table\n",
      "63622615013019 a group of people sitting around a table with a lamp\n",
      "164521087379076 a plant in a pot on a wooden floor next to a lamp\n",
      "210584911715184 a small potted plant on a table in a kitchen\n",
      "130570506286762 a vase with a plant in it\n",
      "218956342449379 a wooden table with a glass top and a wooden chair\n",
      "137484393243235 a red and black potted plant on a sidewalk\n",
      "128345438916152 a man sitting on a bench with a plant in a pot\n",
      "175032836511482 a man sitting on a couch\n",
      "256288219273645 a woman in a black dress standing next to a table with a plant\n",
      "205293592130303 a yellow and blue bicycle parked in the middle of a narrow street\n",
      "142714691655158 a man in a blue shirt and hat is sitting on a chair\n",
      "279076557872563 a yellow wall with a chair and table in front of it\n",
      "59873125215435 a store with many plants and plants in it\n",
      "232246971412933 a man sitting at a table with food on it\n",
      "227997094962531 a horse jumping over a fence in a field\n",
      "197995694620581 a large flower shop with many different types of flowers\n",
      "241627145232838 a bookcase with books\n",
      "15296876901465 a street lamp and a flower pot on a sidewalk\n",
      "85085554261393 a plastic egg box with plants and a plant in it\n",
      "126497643749253 a man sitting on a chair with a glass of water\n",
      "213671056980677 a bathroom with a sink, toilet and shower\n",
      "215894629822554 yellow roses in a vase on a coffee table in a living room\n",
      "76014018405310 a man sitting in a chair in front of a piano\n",
      "255192371020442 a blue chair, a red chair, a potted plant and a flower pot\n",
      "122812664862813 a living room with a television and a table\n",
      "94317064158476 a laptop computer is sitting on a table in front of a wall\n",
      "234066606283447 a television set with a plant on it\n",
      "190657052087305 a green couch with a plant on it\n",
      "60945037422447 a white rose in a black vase\n",
      "79382590223782 a tree is leaning against a building\n",
      "258650855630112 a small dog sitting on a wooden chair\n",
      "106418694234927 a garden center with many plants and plants in pots\n",
      "190560741353015 a large plant with red leaves in a pot\n",
      "136351663754451 a man standing in front of a kitchen counter with a pizza in the oven\n",
      "97633415643635 a bed with blue sheets and a lamp on top\n",
      "48902230820120 a vase with flowers in it\n",
      "232849873900756 a garden with many plants and flowers\n",
      "140854508945284 a vase with red flowers sitting on a bar\n",
      "132407070242704 a table with vegetables and fruit on it\n",
      "264799904287713 a grave with flowers and a plaque\n",
      "255982493234979 a flower pot with plants on the sidewalk\n",
      "8178265365687 a room with a large window and a couch\n",
      "165194226365559 a garden with plants and flowers in the sun\n",
      "91046830938849 a kitchen with a table and a potted plant on it\n",
      "194533229035450 a man sitting on a bike with a basket on it\n",
      "58673502583228 a view of a room with a large window and a door\n",
      "207163083906035 a motorcycle parked on the side of the road\n",
      "123369020941443 a man is standing in front of a plant\n",
      "199336664402585 a woman in a bra and underwear holding a toothbrush\n",
      "160983761673582 a living room with a television, a chair and a plant\n",
      "177185615627555 a building with a sign that says \"the hotel\"\n",
      "20145948819311 two pots with purple flowers hanging from a porch\n",
      "38461628612885 a woman is holding a red and black purse\n",
      "22898005483228 a television set in a living room with a cat on the floor\n",
      "231070913341450 a mirror and a vase with flowers in it\n",
      "104400713272929 a kitchen with a counter top and a coffee maker\n",
      "264288549039916 a man in a blue shirt sitting in a chair\n",
      "63587022096160 a table with a basket of papers and a plant\n",
      "115187556660315 black and white photograph of a man sitting on a wooden deck\n",
      "87636045474916 a man in a blue shirt and hat standing next to a dog\n",
      "183356288397154 a cat sitting in a potted plant on a patio\n",
      "82911146071647 a woman laying in a red chair with a bird on her head\n",
      "5630141576228 a living room with a large couch and a fireplace\n",
      "29923119174337 a table with flowers and food in vases\n",
      "261947701922164 a woman is standing behind a fence with a plant\n",
      "187218400626701 a diploma is on a table\n",
      "229629197607787 a vase of red flowers on a table\n",
      "200101350069313 a man and woman in wedding attire cutting a cake\n",
      "36317005653984 a man standing in front of a table with plants\n",
      "47001044169424 a double decker bus is parked in front of a flower bed\n",
      "38518559883414 a man standing next to a sign\n",
      "134610377650773 a woman sitting at a table with a glass of wine\n",
      "182148881860038 a table with a cake and flowers on it\n",
      "266972791825110 a man in jeans and a shirt standing in front of a plant\n",
      "238661180921868 a small black dog sitting in the grass\n",
      "48980168836808 a garden with many plants and a bird cage\n",
      "4386743698559 a black bird eating an apple from a flower pot\n",
      "230917333581699 a woman with a black shirt on\n",
      "28992905138295 a statue of a monkey sitting on a stone pillar\n",
      "154768637761168 a hanging basket with plants in it\n",
      "137090330411179 a red pot with pink lilies in it\n",
      "279369720874065 a cat sitting on a table next to a vase of flowers\n",
      "109256334482463 a black cat sitting in a plant with its mouth open\n",
      "260725728037592 a woman with an umbrella is standing in front of cactus plants\n",
      "76292514362117 a living room with a couch, coffee table and a lamp\n",
      "231144803151566 a blue mailbox with a flower pot on it\n",
      "240691790369501 a basket of fruit and flowers sits on a table\n",
      "225640591709251 a wooden roof with a snow covered roof and a wooden sign\n",
      "272227301687625 a window with a potted plant in it\n",
      "59956853020785 two chairs and a table with flowers on it\n",
      "13227552723489 a black and white photo of a vase with flowers on it\n",
      "272401039885360 a small plant in a wooden box with a window\n",
      "231969615933880 a kitchen with blue cabinets and a window\n",
      "153142259589407 a red bicycle parked outside a restaurant\n",
      "99222913899809 a bunch of bananas hanging from a plant\n",
      "263344673353696 a large open room with a large table and chairs\n",
      "242676676776643 a glass ball with a plant in it\n",
      "112370828649964 a wooden bench sitting on a grassy area\n",
      "265375404710416 a woman is sitting in a chair in front of a window\n",
      "33957410536275 a large arrangement of flowers in front of a statue\n",
      "125317091008393 a man wearing a suit\n",
      "260410908820050 two blue and yellow toy train cars with flowers in them\n",
      "21452510264508 a woman sitting at a table with a birthday cake\n",
      "64845216599246 a woman is standing outside of a building with a potted plant\n",
      "111704751980333 a vase with red roses on it\n",
      "75940008858115 a flower pot sitting on the side of the road\n",
      "199354169718564 a window with fruit and vegetables on it\n",
      "159534442831352 a wooden bench with a wooden seat and a wooden back\n",
      "219328995777958 pumpkins and flowers in a flower shop window\n",
      "114011180102375 a woman sitting at a table with a glass of wine\n",
      "95627253457443 a man sitting in front of a television with a computer\n",
      "39997814080706 a cat looking out the window\n",
      "185525341962312 a window sill with potted plants and a window\n",
      "201099465727786 a tennis player is playing a match on a court\n",
      "46909337890544 a bathroom with a toilet, a potted plant and a curtain\n",
      "224180077807287 a man in a suit and tie is giving a speech\n",
      "45388731980023 a large building with a clock on the wall\n",
      "59499093293421 a window display with a swan and a vase\n",
      "128650063198669 a vase of purple tulips in front of a window\n",
      "77110030225079 a cat is sitting on a wall with a plant\n",
      "32783897426257 a table with a glass vase and a glass bowl\n",
      "208821024899165 a large elephant statue with a painted face\n",
      "60088522818565 a flower pot with a bird on it\n",
      "151307887613609 a living room with a couch, coffee table and fireplace\n",
      "183027672787951 a bed with a canopy and a potted plant in the corner\n",
      "43619141753761 a red chair and table in a backyard\n",
      "54267705707032 a garden with many pots of flowers and plants\n",
      "254003493255845 a man sitting in a chair looking out the window\n",
      "23474452212980 a cat is sitting on a bench in a garden\n",
      "90275574947790 a table with blue and white flowers and candy\n",
      "69178137804163 a man in a hat and shorts is riding a skateboard\n",
      "264436232132703 a flower bed with red, yellow and green flowers\n",
      "32835691449513 a woman in a blue dress standing next to a cake\n",
      "10281033361380 a table with a bunch of flowers in it\n",
      "155037391780543 a black and white photo of a computer monitor\n",
      "21743643057629 a bottle of carolina reaper sauce sitting on a table\n",
      "83815274007472 a table with bread, vegetables and other food items\n",
      "100566619782301 a window with a potted plant in it\n",
      "161683890991798 a yoga mat in a living room with a window\n",
      "193849814662370 a car with a colorful paint job on the side\n",
      "265612977787176 a black and white photo of a room with a window\n",
      "42601121846481 a group of vases on the sidewalk\n",
      "203555919324956 a man sitting at a table with a plate of food\n",
      "229814323005393 a man is walking down the sidewalk with a dog\n",
      "8191458096403 a man sitting at a table with a flower pot\n",
      "266249074464997 a man is skateboarding down a sidewalk\n",
      "163545021252073 a bonsai tree is on top of a table\n",
      "30509949971773 a chair in front of a window with plants and a table\n",
      "190033407942634 a black cat sitting in a flower pot\n",
      "203495061389940 a table with flowers and a vase on it\n",
      "22781371134483 a bathroom with a sink, toilet and mirror\n",
      "267113210806661 a living room with a couch and a kitchen\n",
      "74118847432671 a table with a lamp, a vase and a plant\n",
      "90847130375176 a vase filled with flowers on a table outside\n",
      "4771184079549 a vase with yellow flowers sitting on a table\n",
      "53941347705424 a man and woman walking on a sidewalk near a bus stop\n",
      "3567558486044 a red building with a sign that says chinese restaurant\n",
      "167294243031185 a living room with a couch, chairs and a table\n",
      "128868112505273 a bedroom with a bed, a mirror and a large plant\n",
      "137590984245846 a young boy sitting on the couch with a christmas tree\n",
      "209359616538389 a bench and a potted plant in the yard\n",
      "116173863141786 a kitchen with a table and chairs in it\n",
      "131785974206791 a man and woman in wedding attire standing next to each other\n",
      "100109604846752 a display of plants and pots in a store\n",
      "26895051593157 a street with a building in the background and a car in the middle\n",
      "139399784810922 a vase with flowers sitting on a window sill\n",
      "206770232071642 a living room with a couch, chair and a television\n",
      "107875633375192 a man in a blue shirt standing in front of a potted plant\n",
      "152321141044690 a kitchen counter with a sink, a microwave and a stove\n",
      "202554887567193 a small table with a plant on it and a small box\n",
      "24771094149956 a blue bench sitting on the sidewalk next to a flower pot\n",
      "69626631326801 a man playing guitar at a picnic table\n"
     ]
    }
   ],
   "source": [
    "for p in visda[\"real_train\"][\"metadata\"][27000:27200]:\n",
    "    print(p[\"image_id\"], p[\"blip2_cap\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534286fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
