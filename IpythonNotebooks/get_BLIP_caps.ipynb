{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df387e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26eca3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab77fd1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a photo of a group of people sitting under umbrellas\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(\"cuda\")\n",
    "\n",
    "img_url = '/newdata/tarun/datasets/WebVision/GeoILSVRC/asia/test/umbrella/16046885830.jpg' \n",
    "raw_image = Image.open(img_url).convert('RGB')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "36071904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conditional image captioning\n",
    "text = \"a photo of\"\n",
    "inputs = processor(raw_image, text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# inputs[\"max_length\"] = 20\n",
    "# inputs[\"min_length\"] = 3\n",
    "inputs[\"return_dict\"] = True\n",
    "\n",
    "out = model(**inputs)\n",
    "# print(processor.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fc4c518b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['loss', 'decoder_logits', 'image_embeds', 'last_hidden_state'])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eb6e6d70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 577, 768])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[\"image_embeds\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1910fe83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(384, 256)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_image.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25cae3ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pixel_values', 'input_ids', 'attention_mask', 'max_length', 'min_length'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ff9143bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 384, 384])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['pixel_values'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fe7760cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BlipVisionEmbeddings(\n",
       "  (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vision_model.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e01d3e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BlipForConditionalGeneration' object has no attribute 'cls'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcls\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/hface/lib/python3.9/site-packages/torch/nn/modules/module.py:1207\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1205\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1206\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1207\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1208\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BlipForConditionalGeneration' object has no attribute 'cls'"
     ]
    }
   ],
   "source": [
    "model.cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "068d598f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BlipVisionConfig {\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"dropout\": 0.0,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_size\": 768,\n",
       "  \"image_size\": 384,\n",
       "  \"initializer_factor\": 1.0,\n",
       "  \"initializer_range\": 1e-10,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"model_type\": \"blip_vision_model\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_channels\": 3,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"patch_size\": 16,\n",
       "  \"projection_dim\": 512,\n",
       "  \"transformers_version\": \"4.26.1\"\n",
       "}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.models.blip.configuration_blip.BlipVisionConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39f7b1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce7f5ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import List\n",
    "# from collections import Counter\n",
    "\n",
    "def generate_ngrams(text_list, n):\n",
    "    \"\"\"\n",
    "    Generate the number of unique n-grams from a list of strings.\n",
    "\n",
    "    :param text_list: List of input strings.\n",
    "    :param n: The number of elements in each n-gram.\n",
    "    :return: Number of unique n-grams.\n",
    "    \"\"\"\n",
    "    ngrams = []\n",
    "    for text in text_list:\n",
    "        words = text.split()\n",
    "        for i in range(len(words) - n + 1):\n",
    "            ngram = ' '.join(words[i:i + n])\n",
    "            ngrams.append(ngram)\n",
    "    \n",
    "    unique_ngrams = set(ngrams)\n",
    "    return len(unique_ngrams)\n",
    "\n",
    "# # Example usage\n",
    "# text_list = [\"this is a test\", \"this is another test\", \"yet another test\"]\n",
    "# bigrams_count = generate_ngrams(text_list, 2)\n",
    "# trigrams_count = generate_ngrams(text_list, 3)\n",
    "\n",
    "# bigrams_count, trigrams_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f66b441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeoPlaces,asia\n",
      "llm_cap_llama_13b, BiGrams:306829, TriGrams:557686, len:10.398668092190999\n",
      "\n",
      "blip2_cap, BiGrams:55041, TriGrams:137778, len:9.45704012212701\n",
      "\n",
      "GeoPlaces,usa\n",
      "llm_cap_llama_13b, BiGrams:464702, TriGrams:854670, len:12.658952332828028\n",
      "\n",
      "blip2_cap, BiGrams:61278, TriGrams:137890, len:9.367564987928809\n",
      "\n",
      "GeoImnet,asia\n",
      "llm_cap_llama_13b, BiGrams:160293, TriGrams:253329, len:9.857949419399901\n",
      "\n",
      "blip2_cap, BiGrams:34160, TriGrams:80224, len:9.23782776985536\n",
      "\n",
      "GeoImnet,usa\n",
      "llm_cap_llama_13b, BiGrams:370761, TriGrams:631261, len:10.902135461047848\n",
      "\n",
      "blip2_cap, BiGrams:68864, TriGrams:161560, len:9.455496165465954\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datasets = [\"GeoPlaces\", \"GeoImnet\"]\n",
    "domains = [\"asia\", \"usa\"]\n",
    "\n",
    "for dataset in datasets:\n",
    "    for domain in domains:\n",
    "        print(\"{},{}\".format(dataset, domain))\n",
    "        meta = json.load(open(f\"../metadata/{dataset.lower()}.json\"))[f'{domain}_train']['metadata']\n",
    "\n",
    "        for cap_src in [\"llm_cap_llama_13b\", \"blip2_cap\"]:\n",
    "            caps = [m[cap_src] for m in meta]\n",
    "            lens = [len(c.split(\" \")) for c in caps]\n",
    "            mean_len = sum(lens)/len(lens)\n",
    "            bigrams = generate_ngrams(caps, 2)\n",
    "            trigrams = generate_ngrams(caps, 3)\n",
    "            print(\"{}, BiGrams:{}, TriGrams:{}, len:{}\\n\".format(cap_src, bigrams, trigrams, mean_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8c47a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rawtags = [m[\"tags\"].replace(\",\",\" \") + m[\"caption\"] + m[\"description\"] for m in meta]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5fd380b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1509130, 2097825)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_ngrams(rawtags,2), generate_ngrams(rawtags, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8af44b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
