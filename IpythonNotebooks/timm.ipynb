{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8d45a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "216d6d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(model_name, pretrained=True):\n",
    "    \n",
    "    model = timm.create_model(model_name, pretrained=pretrained)\n",
    "    params = sum([p.numel() for p in model.parameters() if p.requires_grad==True])\n",
    "    params /= 1e6\n",
    "    # print(\"Number of parameters in {}: {:.02f}M\".format(model_name, params))\n",
    "    return model, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86e1317d",
   "metadata": {},
   "outputs": [],
   "source": [
    "swin = [b for b in timm.list_models() if \"swinv2_base\" in b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69d81fb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['swinv2_base_window8_256',\n",
       " 'swinv2_base_window12_192',\n",
       " 'swinv2_base_window12to16_192to256',\n",
       " 'swinv2_base_window12to24_192to384',\n",
       " 'swinv2_base_window16_256']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7be30cc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40e35e67e8cf472e8d2fe66b1175221a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/357M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, params = create_model(\"swinv2_base_window16_256\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e245db63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87.918816"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bfcc1d31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SwinTransformerV2(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (layers): Sequential(\n",
       "    (0): SwinTransformerV2Stage(\n",
       "      (downsample): Identity()\n",
       "      (blocks): ModuleList(\n",
       "        (0): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=4, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): Identity()\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (act): GELU(approximate=none)\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (1): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=4, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.004)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (act): GELU(approximate=none)\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.004)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): SwinTransformerV2Stage(\n",
       "      (downsample): PatchMerging(\n",
       "        (reduction): Linear(in_features=512, out_features=256, bias=False)\n",
       "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (blocks): ModuleList(\n",
       "        (0): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=8, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=256, out_features=768, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.009)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (act): GELU(approximate=none)\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.009)\n",
       "        )\n",
       "        (1): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=8, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=256, out_features=768, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.013)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (act): GELU(approximate=none)\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.013)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): SwinTransformerV2Stage(\n",
       "      (downsample): PatchMerging(\n",
       "        (reduction): Linear(in_features=1024, out_features=512, bias=False)\n",
       "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (blocks): ModuleList(\n",
       "        (0): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.017)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate=none)\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.017)\n",
       "        )\n",
       "        (1): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.022)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate=none)\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.022)\n",
       "        )\n",
       "        (2): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.026)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate=none)\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.026)\n",
       "        )\n",
       "        (3): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.030)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate=none)\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.030)\n",
       "        )\n",
       "        (4): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.035)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate=none)\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.035)\n",
       "        )\n",
       "        (5): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.039)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate=none)\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.039)\n",
       "        )\n",
       "        (6): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.043)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate=none)\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.043)\n",
       "        )\n",
       "        (7): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.048)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate=none)\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.048)\n",
       "        )\n",
       "        (8): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.052)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate=none)\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.052)\n",
       "        )\n",
       "        (9): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.057)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate=none)\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.057)\n",
       "        )\n",
       "        (10): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.061)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate=none)\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.061)\n",
       "        )\n",
       "        (11): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.065)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate=none)\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.065)\n",
       "        )\n",
       "        (12): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.070)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate=none)\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.070)\n",
       "        )\n",
       "        (13): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.074)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate=none)\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.074)\n",
       "        )\n",
       "        (14): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.078)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate=none)\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.078)\n",
       "        )\n",
       "        (15): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.083)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate=none)\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.083)\n",
       "        )\n",
       "        (16): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.087)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate=none)\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.087)\n",
       "        )\n",
       "        (17): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.091)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate=none)\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.091)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): SwinTransformerV2Stage(\n",
       "      (downsample): PatchMerging(\n",
       "        (reduction): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (blocks): ModuleList(\n",
       "        (0): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=32, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.096)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate=none)\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.096)\n",
       "        )\n",
       "        (1): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=32, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.100)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate=none)\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.100)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (head): ClassifierHead(\n",
       "    (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Identity())\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (fc): Linear(in_features=1024, out_features=1000, bias=True)\n",
       "    (flatten): Identity()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa65bd90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
